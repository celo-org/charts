---
image:
  repository: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-node
  tag: v1.7.4
  pullPolicy: Always

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# Number of op-node replicas. Must be 1 if sequencer is enabled
replicaCount: 1

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

updateStrategy:
  type: RollingUpdate

statefulset:
  # Annotations to add to the statefulset
  annotations: {}
  # Annotations to add to the pod template
  podAnnotations: {}

terminationGracePeriodSeconds: 300

podSecurityContext: {}
  # fsGroup: 2000

securityContext:
  capabilities:
    drop:
    - ALL
  allowPrivilegeEscalation: false
  privileged: false
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000
  # runAsGroup: 1000

## Override op-node command (can be templated)
command: []

## Extra op-node arguments (can be templated)
extraArgs: []

## Extra init containers, can be templated
initContainers: []

## Sidecar containers, can be templated
sidecarContainers: []

## Services config
services:
  p2p:
    enabled: true
    type: NodePort
    loadBalancerIPs: []
    clusterIPs: []
    nodePorts: []
    port: 9222
    # it's better to set nodePort equal to .Values.config.node.p2p.port when the svc type is "NodePort"
    # nodePort: 9222
    annotations: {}
    publishNotReadyAddresses: true
  rpc:
    sharedServiceEnabled: true
    individualServiceEnabled: true
    type: ClusterIP
    port: 9545
    annotations: {}
    publishNotReadyAddresses: false
  metrics:
    enabled: true
    type: ClusterIP
    port: 7300
    annotations: {}
    publishNotReadyAddresses: true

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
    # cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts: []
    # - host: op-node.examplerpc.com
    #   paths:
    #     - path: /
    #       pathType: ImplementationSpecific
  tls: []
    # - secretName: op-node-tls
    #   hosts:
    #     - op-node.examplerpc.com

# Create Prometheus Operator serviceMonitor
serviceMonitor:
  enabled: false
  # interval: 10s
  # scrapeTimeout: 2s
  # honorLabels: true
  # relabelings: []
  # metricRelabelings: []

# .livenessProbe.exec.command can also be in a templated string format
# If not using exec, set exec: null
livenessProbe:
  enabled: true
  initialDelaySeconds: 60
  periodSeconds: 30
  successThreshold: 1
  failureThreshold: 3
  timeoutSeconds: 5
  exec: null
  httpGet:
    path: /healthz
    port: rpc

# .readinessProbe.exec.command can also be in a templated string format
# If not using exec, set exec: null
readinessProbe:
  enabled: false
  # initialDelaySeconds: 60
  # periodSeconds: 30
  # successThreshold: 1
  # failureThreshold: 3
  # timeoutSeconds: 5
  # exec: null
  # httpGet:
  #   path: /healthz
  #   port: rpc

# Recommended readinessProbe when sequencing enabled (required for HA)
# readinessProbe:
#   enabled: true
#   initialDelaySeconds: 2
#   periodSeconds: 4
#   successThreshold: 1
#   failureThreshold: 2
#   timeoutSeconds: 2
#   exec:
#     command:
#       - /bin/sh
#       - -c
#       - |
#         response=$(wget --quiet --timeout=1 --header="Content-Type: application/json" --post-data='{"jsonrpc":"2.0","method":"admin_sequencerActive","id":1}' -O- http://localhost:{{ .Values.config.port }})
#         echo $response | jq -e '.result == true' > /dev/null || exit 1

resources: {}

affinity: {}

nodeSelector: {}

tolerations: []

persistence:
  type: pvc
  pvc:
    size: 1Gi
    accessMode: ReadWriteOnce
    storageClass: ""
    annotations:
      # resize.topolvm.io/increase: 1Gi
      # resize.topolvm.io/storage_limit: 10Gi
      # resize.topolvm.io/threshold: 10%
      # resize.topolvm.io/inodes-threshold: 5%
  mountPath: ""                 # mount path for container fs, leave blank to use value from .Values.rollup.config

## initContainers configuration
init:
  image:
    repository: alpine
    tag: 3.19
    pullPolicy: IfNotPresent
  rollup:
    enabled: false
    url: ""

secrets:
  # REQUIRED: JWT for communication with op-geth. In case of multiReplica, comma separated list of JWTs (or shared)
  # Either provide the secret name and key or the value directly. If value is not empty, it will have precedence over the secret.
  jwt:
    value: ""
    secretName: ""
    secretKey: ""
  # Private Key for signing the sequencing messages. Either provide the secret name and key or the value directly. If value is not empty, it will have precedence over the secret.
  sequencerSigningKey:
    value: ""
    secretName: ""
    secretKey: ""
  # REQUIRED: Private Key for the p2p. Comma separated list of keys if multiple replicas
  # Either provide the secret name and key or the value directly. If value is not empty, it will have precedence over the secret.
  p2pKeys:
    value: ""
    secretName: ""
    secretKey: ""
  # REQUIRED: L1 URL. Either provide the secret name and key or the value directly. If value is not empty, it will have precedence over the secret.
  l1Url:
    value: ""                              # eth L1 node rpc url, can also be ws://
    secretName: ""
    secretKey: ""
  # L1 Beacon URL. Either provide the secret name and key or the value directly. If value is not empty, it will have precedence over the secret.
  l1BeaconUrl:
    value: ""                             # endpoint for L1 beacon node, used for retrieving EIP-4844 blob transactions
    secretName: ""
    secretKey: ""

## Main op-node config
config:
  jwt: ""                              # REQUIRED for communication with op-geth. In case of multiReplica, comma separated list of JWTs (or shared)
  network: op-mainnet                  # which network to use
  port: 9545
  log:
    level: info
    format: json
    color: false
  syncmode: ""                         # can be "execution-layer" if you are using "snap" syncmode on execution layer
  enableAdmin: false                   # enable admin API
  safedbPath: ""                       # File path used to persist safe head update data. Disabled if not set or empty
  l1:
    trustrpc: false                                       # enable if you trust L1 provider and want to fetch data from it faster
    rpckind: any                                          # Valid options: alchemy, quicknode, infura, parity, nethermind, debug_geth, erigon, basic, any, standard
  l2:
    url: http://op-geth-authrpc:8551         # authrpc addr of op-geth L2 node, can also be ws://
    protocol: ""                             # Used for multiReplica. Leave url empty if using multiReplica
    namePattern: ""                          # Used for multiReplica. Set to the op-geth sts name pattern. Example: "op-geth-validator-authrpc"
    port: ""                                 # Used for multiReplica. Authrpc port of op-geth L2 node with multiReplica
  rollup:
    config: "/celo"                         # path to rollup config json folder
    loadProtocolVersions: true         # load superchain contract
    halt: ""                           # possible values: major, minor, patch, none
  sequencer:
    enabled: false                     # enable op-node as sequencer
    stopped: false                     # initialize the sequencer in a stopped state
    l1Confs: 5                         # number of L1 blocks to keep distance from the L1 head as a sequencer for picking an L1 origin
    useFinalized: false                # Enable use of only finalized L1 blocks as L1 origin. Overwrites the value of `l1Confs`
    maxSafeLag: 0                      # maximum number of L2 blocks for restricting the distance between L2 safe and unsafe. Disabled if 0
  verifier:
    l1Confs: 0                         # number of L1 blocks to keep distance from the L1 head before deriving L2 data from
  conductor:
    enabled: false                     # Enable the conductor service
    rpc: http://127.0.0.1:8547         # Conductor service rpc endpoint
    rpcTimeout: 1s                     # Conductor service rpc timeout
  altda:
    enabled: false                     # enable altda
    daServer: ""                       # altda data availability server
    daService: true                    # Use DA service type where commitments are generated by altda server
    verifyOnRead: false                # verify input data matches the commitments from the DA storage service
  metrics:
    enabled: false                     # enable metrics server
    port: 7300
  p2p:
    nat: false                         # use NAT to get external IP
    port: 9222
    useHostPort: false                 # use hostPort for p2p traffic instead of dedicated k8s svc
    bootnodes: []                      # override bootnodes
    static: []                         # static peers
    netrestrict: []                    # restrict network access to specific IPs
    noDiscovery: false                 # disable peer discovery
    keys: ""                           # comma separated list of keys to use for p2p
    sequencer:
      key: ""                            # sequencer key

## GCS Rollup sync config
gcsConfig:
  image:
    repository: gcr.io/google.com/cloudsdktool/google-cloud-cli
    tag: latest
    pullPolicy: IfNotPresent
  # local storage config
  local:
    # datadir containing the state you want to upload (can be templated)
    datadir: "{{ .Values.config.datadir }}"
    # this file marks node as already initialized from snapshot
    # should be placed outside of the datadir you are uploading
    initializedFile: "{{ .Values.config.datadir }}/.initialized"
  # remote storage config
  remote:
    # Assuming your S3 bucket name is `my-snapshot-bucket` and base directory name is Helm release name
    # snapshot will be uploaded to {{ .baseUrl }}/upload directory
    baseUrl: my-snapshot-bucket/{{ .Release.Name }}
    # Any S3-compatible object storage service should be supported, but has only been tested with GCS.
    # I.e. Amazon S3, MinIO, DigitalOcean Spaces, CloudFlare R2.
    # endpointUrl: https://storage.googleapis.com
    endpointUrl: ""
    # How to create access key
    # AWS S3 https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html
    # GCS    https://cloud.google.com/storage/docs/authentication/managing-hmackeys#create

initFromGCS:
  # enable initContainer
  enabled: false
  # re-download rollup from GCS on every pod start
  force: false
